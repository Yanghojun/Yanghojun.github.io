---
layout: article
title:  "대표 딥러닝 모델 정리"
category: [인공지능]
tag: [머신러닝, 딥러닝, 시계열 데이터, 순환 신경망]
permalink: /RepresentativeDeeplearningModel/
show_author_profile: true
aside:
    toc: true
sidebar:
    nav: "study-nav"
---

# 순환 신경망 (RNN)

[MIT 강의영상 및 정리](https://www.notion.so/MIT-ba17243865364c8696dbfdd9fe3c265c)

[Illustrated Guide to LSTM's and GRU's: A step by step explanation](https://www.youtube.com/watch?v=8HyCNIVRbSU)

[Lecture 10 | Recurrent Neural Networks](https://www.youtube.com/watch?v=6niqTuYFZLQ&t=3759s)

## RNN

- 시퀀스 길이에 관계없이 인풋, 아웃풋을 받아드릴 수 있는 네트워크 구조여서 **매우 유연함**
    
    RNN 그림
    ![](/images/2022-02-10-23-18-11.png)
    
    - 가중치 매개변수 행렬은 위 그림에서 확인할 수 있듯 3개
        - $W_{hh}$, $W_{xh}$, $W_{hy}$
    - Hidden state라는게 왜 있는걸까?
        - 내 생각: $W_{xh}x_t$에 대해 일단 비선형성을 주긴 할 것임 → tanh를 비선형 함수로 사용 → $tanh(W_{xh}x_t)$인데 recurrent 함을 주기 위해 **과거와 연관되는** 값을 하나 추가시켜 줄 것임. 그것이 $h_{t-1}$
    - 하이퍼볼릭탄젠트 함수 사용하는 이유
        - 하이퍼볼릭탄젠트란?
            - 쌍곡선 함수라고도 불리며 식은 다음과 같다
                
                $sinhx = {e^x - e^{-x} \over 2} \\
                coshx = {e^x + e{-x} \over 2} \\
                tanhx = {sinhx \over coshx} = {e^x - e^{-x} \over e^x + e^{-x}}$
                
            
            ![](/images/2022-02-10-23-18-26.png)
            
    
    Gradient 그림
    
    ![](/images/2022-02-10-23-18-34.png)
    
    - W가 스칼라 값이라고 가정했을 때 W가 1보다 작으면 빨간색 화살표가 뒤로 갈수록 계속 값이 줄어듬(W값은 항상 똑같은 값이기 때문에 계속 똑같은 값이 곱해짐). Vanishing gradients
    - W 값이 1보다 크면 Exploding gradients
        - W는 사실 스칼라가 아니라 매트릭스기 때문에 Largest singular value > 1이면 Exploding, Largest singular value < 1 이면 Vanishing
    - Exploding은 gradient clipping 으로 해결 가능. Vanishing은 RNN 구조 자체를 바꿔야함
    
    RNN 오차 역전파법 그림
    
    ![](/images/2022-02-10-23-18-44.png)
    

## LSTM

- RNN의 Vanishing gradient problem 해결을 위한 모델
    
    
    RNN, LSTM 그림
    

![](/images/2022-02-10-23-19-09.png)

![](/images/2022-02-10-23-19-13.png)

LSTM 수식

![](/images/2022-02-10-23-19-22.png)

- $f_t$는 **과거 정보 살릴지 여부**
    - 시그모이드 함수의 범위(0~1)을 통해 과거 정보를 살릴지(1) 없앨지(0) 결정

- 참고사이트
    - [https://ratsgo.github.io/natural language processing/2017/03/09/rnnlstm/](https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/)
    - [https://wikidocs.net/22888](https://wikidocs.net/22888)

# 이해를 위한 그림 (매우 중 요)

![](/images/2022-02-10-23-19-36.png)

아주 핵심적인 그림이당

![](/images/2022-02-10-23-19-47.png)

## 선우형이 추천해준 순환신경망 개념 블로그

[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

## ANN CNN RNN 비교 영상

[Recurrent Neural Networks | RNN LSTM Tutorial | Why use RNN | On Whiteboard | Compare ANN, CNN, RNN](https://www.youtube.com/watch?v=KBftoy0DPxI&ab_channel=BinodSumanAcademy)

### 발전과정

[Attention](https://www.notion.so/Attention-ee1d614d57394306b555bbf1a46bf96b)

[Transformer](https://www.notion.so/Transformer-588e0039e1814f4faf45e1919e247a52)

[합성곱 신경망 (CNN)](https://www.notion.so/CNN-d37935874a854172925a885df14a386a)